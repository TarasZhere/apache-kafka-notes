---
title: 8a. Producer Ack and Topic Durability
updated: 2023-09-23 03:53:33Z
created: 2023-09-10 18:10:48Z
---

# Producer Acknowledgements (acks)

<center style="padding: 0 10%">

![Screenshot 2023-09-10 at 12.35.16 PM.png](./_resources/Screenshot%202023-09-10%20at%2012.35.16%20PM.png)

</center>

Producers can choose to receive **Acks** when writing data:

-   `acks = 0`: The Producer will not wait for acknowledgment
    -   **Possible loss of data**
-   `acks = 1`: The Producer will wait for acknowledgment from the leader
    -   **Limited loss of data**
    -   Data loss can happen during leader re-election if the replica stream of data is not ISR (in-synch replica)<sup>[[1]](#1)</sup>.
-   `acks = all`: The Producer will wait for leader + replicas `acks`
    -   **No loss of data**

## Acks=0

-   If the broker goes offline or has an exception, we will not know it and will lose the data
-   Useful for data in which it is ok to lose some messages, such as metric collection
-   Produces the highest throughput setting because the network is minimized

## Acks=1

> Default from Kafka `v1.0` to `v2.8`

-   Leader's response is requested, but replication is not guaranteed to have the same messages (In-Sync Replica)
-   We have data loss when the leader goes offline, and the data is not replicated yet or correctly
-   if ack is not received, the producer may retry again

## Acks=all (or -1)

When all In-Sync Replicas accept messages, then the messages are considered written successfully.

> Default from kafka `v3.0+`

The **ack** is done via Raft Consensus Algorithms.

# Producer `ack=all` & `min.in sync.replicas`

The leader of a partition checks to see if there are enough **ISR** for safety writing messages (controlled by the broker by the `min.in sync.replicas`)

-   `min.in sync.replicas=1` only the **leader** needs to successfully ack
-   `min.in sync.replicas=2` only one **leader** and a **follower** need to successfully ack

For the safest and highest guarantee, it is recommended to have `min.insync.replicas=n` where `n = #replicas - 1`. If we have a replication factor of 3 and `min.in sync.replicas=3` Kafka kinda loses its purpose

# Kafka Topic Availability

**Availability: (Consider a replication factor of 3)**

-   `acks=0` or `acks=1`: if one partition is up and consider ISR, the topic will be available for writes
-   `acks=all`:
    -   `min.insync.replicas=1` (default): the topic must have at least one partition up as an ISR (including the leader), and so we can collaborate with brokers going down
    -   `min.insync.replicas=2`: The topic must have at least 2 ISR up. We can tolerate at most one broker going down, and we can guarantee at least two replicas
    -   `min.in sync.replicas=3`: This would not make sense since we cannot tolerate any broker going down
    -   in summary, when `acks=all` with `replication-factor=N` and `min.insync.replicas=M`, we can tolerate `M-N brokers going down` for topic availability purposes.

`acks=all` and `min.insync.replicas=2` is the most popular option for **durability** and **availability**

# Kafka Topic Durability

Now we can better understand **topic durability**. If we have a replication factor of 3, we **can stand** two brokers without losing data.

> By general rule, if we choose the replication factor of `n`, then Kafka can permanently lose `n-1` brokers and recover your data.

 <center style="padding: 0 15%">

![Screenshot 2023-09-10 at 11.15.25 AM.png](/_resources/Screenshot%202023-09-10%20at%2011.15.25%20AM.png)

</center>

---

<small>

<a name="1"></a> [1] - Happens when the replica is not fast enough to replicate from the leader and the leader changes or dies

</small>
