---
title: 8e. Messages compression
updated: 2023-09-23 03:53:45Z
created: 2023-09-21 22:29:01Z
---

# Messages compression at the Producer Level

Producer usually sends data through JSON format (string). In this case, it is possible to compress the data before the producer sends it. The compression happens at the producer level.
It does not require any config at the broker and consumer level.
Compression types:

-   none (default)
-   gzip
-   lz4
-   snappy
-   zstd (Kafka `v2.1`)

Compression is more effective for bigger batches of data sent.
[Cloudflare's article](https://blog.cloudflare.com/squeezing-the-firehose/) has some performance engineering. Look at the article.

<center style="padding: 0 10%">

![55b9546250d3fb4456d95809c27dc69c.png](../_resources/55b9546250d3fb4456d95809c27dc69c.png)

</center>

# Messages Compression

<center style="padding: 0 10%">

|               Advantages               |                       Disadvantages                       |
| :------------------------------------: | :-------------------------------------------------------: |
|       Much producer request size       | producer must commit some CPU cycles to compress the data |
| Faster to transfer data across network | Consumer must decompress the data and use some CPU cycles |
|        Better Kafka throughput         |
|    Better disk utilization in kafka    |

</center>

Overall, try to use `snappy` or `lz4` for optimal speed /compression ratio, and consider tweaking `linger.ms` and `batch.size` to have bigger batches and, therefore, more compression and higher throughput.

# Messages Compression at Producer Level

Producer usually sends data through JSON format (string). In this case, it is possible to compress the data before the producer sends it. The compression happens at the producer level.
It does not require any config at the broker and consumer level.
Compression types:

-   none (default)
-   gzip
-   lz4
-   snappy
-   zstd (Kafka `v2.1`)

Compression is more effective for bigger batches of data sent.
[Cloudflare's article](https://blog.cloudflare.com/squeezing-the-firehose/) has some performance engineering. Look at the article.

<center style="padding: 0 10%">

![55b9546250d3fb4456d95809c27dc69c.png](../_resources/55b9546250d3fb4456d95809c27dc69c.png)

</center>

# Messages Compression

<center style="padding: 0 10%">

|               Advantages               |                       Disadvantages                       |
| :------------------------------------: | :-------------------------------------------------------: |
|       Much producer request size       | producer must commit some CPU cycles to compress the data |
| Faster to transfer data across network | Consumer must decompress the data and use some CPU cycles |
|        Better Kafka throughput         |
|    Better disk utilization in kafka    |

</center>

Overall, try to use `snappy` or `lz4` for optimal speed /compression ratio, and consider tweaking `linger.ms` and `batch.size` to have bigger batches and, therefore, have more compression and higher throughput.

<center>

### ⚠️ Use compression on production ⚠️

</center>

## `linger.ms` and `batch.size` Producer settings

By default the producer is going to send the batch as soon as possible.

-   it will have at most `max.in.flight.requests.per.connection=5`, meaning up to 5 batches can be in flight.
-   After this if more batchesmust be sent in while others are in flights, Kafka is smart and it will start batching them before the next batch send.

This smart batches help increasing throughput and while mantaining very low latency.

### Settings to increase batching mechanism

-   `linger.ms` (default 0): How long to wait until we send a batch. Adding a small number such 5ms helps add more messages in the batch at expenses of latency
-   `batch.size`: if a batch is filled before the `linger.ms`, increase the batch size

### `batch.size` (default 16KB)

-   this setting indicates the max number of KB that will be added to a batch.
-   Increasing the batch size up to 32 or 64 KB will often increase the throughput, compression, and efficency of requests.
-   if a single message is bigger than the `batch.size` then it is not batched and sent right away.
-   A batch is allocated per partition, so make sure that you don't set to a number too high otherwise memory will be wasted.
-   You can monitor the average batchsize metrics with the **Kafka Producer Metrics**

### High Throughput Producer

-   increase `linger.ms` and the producer will wait few milliseconds for the batches to fill up before sending them
-   If you have full batches and memory to spare than it is good partice to increase the `batche.size`
-   Introduce compression at producer level and set brokers `compression.type=producer` <sup>[[1]](#1)</sup> so that brokesr do not waste time to compress the batch

# Messages Compression at Broker/Topic Level

<a name="1"></a>

There is also a setting that allows set compression at the broker level (all topics) or topic level

-   `compression.type=producer` (default), the broker takes the batch of data from the Kafka client and directly stores it in disk without re-compressing
-   `compression.type=none` all batches are decompressed by the broker
-   `compression.type=lz4` (example)
    -   if the compression type matches the producer, the data will be stored as it is.
    -   if it is different, then first, the data is decompressed by the broker and then re-compressed with the specified compression type

<center>

### ⚠️ If you enable broker-level compression, it will use extra CPU cycles ⚠️

</center>
